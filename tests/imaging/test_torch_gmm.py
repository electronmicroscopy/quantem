import numpy as np
import pytest
import torch

from quantem.imaging.lattice import TorchGMM


class TestTorchGMMInitialization:
    """Test TorchGMM initialization and parameter setup."""

    def test_init_default_params(self):
        """Test initialization with default parameters."""
        gmm = TorchGMM(n_components=3)
        assert gmm.n_components == 3
        assert gmm.covariance_type == "full"
        assert gmm.means_init is None
        assert gmm.tol == 1e-4
        assert gmm.max_iter == 200
        assert gmm.reg_covar == 1e-6
        assert gmm.dtype == torch.float32

    def test_init_custom_params(self):
        """Test initialization with custom parameters."""
        means = np.random.randn(2, 3)
        gmm = TorchGMM(
            n_components=2,
            means_init=means,
            tol=1e-5,
            max_iter=100,
            reg_covar=1e-5,
            dtype=torch.float64,
        )
        assert gmm.n_components == 2
        assert gmm.means_init.shape == (2, 3)
        assert gmm.tol == 1e-5
        assert gmm.max_iter == 100
        assert gmm.reg_covar == 1e-5
        assert gmm.dtype == torch.float64

    def test_init_unsupported_covariance_type(self):
        """Test that unsupported covariance types raise NotImplementedError."""
        with pytest.raises(NotImplementedError, match="Only 'full' covariance_type"):
            TorchGMM(n_components=2, covariance_type="diag")

    def test_init_fitted_attributes_none(self):
        """Test that fitted attributes are None before fitting."""
        gmm = TorchGMM(n_components=2)
        assert gmm.means_ is None
        assert gmm.covariances_ is None
        assert gmm.weights_ is None

    @pytest.mark.parametrize("device", ["cpu", "cuda"])
    def test_init_device_selection(self, device):
        """Test device selection (skip cuda if not available)."""
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("CUDA not available")

        gmm = TorchGMM(n_components=2, device=device)
        assert gmm.device == device


class TestTorchGMMTensorConversion:
    """Test tensor conversion utilities."""

    def test_to_tensor_from_numpy(self):
        """Test conversion from NumPy array."""
        gmm = TorchGMM(n_components=2)
        x = np.random.randn(10, 2)
        tensor = gmm._to_tensor(x)

        assert isinstance(tensor, torch.Tensor)
        assert tensor.dtype == torch.float32
        assert tensor.shape == (10, 2)
        assert np.allclose(tensor.cpu().numpy(), x, atol=1e-6)

    def test_to_tensor_from_torch(self):
        """Test conversion from existing torch tensor."""
        gmm = TorchGMM(n_components=2, device="cpu", dtype=torch.float64)
        x = torch.randn(10, 2, dtype=torch.float32)
        tensor = gmm._to_tensor(x)

        assert isinstance(tensor, torch.Tensor)
        assert tensor.dtype == torch.float64
        assert tensor.device.type == "cpu"

    def test_to_tensor_from_list(self):
        """Test conversion from list."""
        gmm = TorchGMM(n_components=2)
        x = [[1.0, 2.0], [3.0, 4.0]]
        tensor = gmm._to_tensor(x)

        assert isinstance(tensor, torch.Tensor)
        assert tensor.shape == (2, 2)


class TestTorchGMMFit:
    """Test TorchGMM fitting functionality."""

    @pytest.fixture
    def simple_2d_data(self):
        """Generate simple 2D data with two clear clusters."""
        np.random.seed(42)
        cluster1 = np.random.randn(100, 2) * 0.5 + np.array([0, 0])
        cluster2 = np.random.randn(100, 2) * 0.5 + np.array([5, 5])
        return np.vstack([cluster1, cluster2])

    @pytest.fixture
    def simple_3d_data(self):
        """Generate simple 3D data with three clear clusters."""
        np.random.seed(42)
        cluster1 = np.random.randn(50, 3) * 0.3 + np.array([0, 0, 0])
        cluster2 = np.random.randn(50, 3) * 0.3 + np.array([3, 3, 3])
        cluster3 = np.random.randn(50, 3) * 0.3 + np.array([-3, 3, -3])
        return np.vstack([cluster1, cluster2, cluster3])

    def test_fit_returns_self(self, simple_2d_data):
        """Test that fit returns self for method chaining."""
        gmm = TorchGMM(n_components=2)
        result = gmm.fit(simple_2d_data)
        assert result is gmm

    def test_fit_sets_attributes(self, simple_2d_data):
        """Test that fit sets means_, covariances_, and weights_."""
        gmm = TorchGMM(n_components=2)
        gmm.fit(simple_2d_data)

        assert gmm.means_ is not None
        assert gmm.covariances_ is not None
        assert gmm.weights_ is not None
        assert isinstance(gmm.means_, np.ndarray)
        assert isinstance(gmm.covariances_, np.ndarray)
        assert isinstance(gmm.weights_, np.ndarray)

    def test_fit_correct_shapes(self, simple_2d_data):
        """Test that fitted parameters have correct shapes."""
        n_components = 2
        n_features = simple_2d_data.shape[1]

        gmm = TorchGMM(n_components=n_components)
        gmm.fit(simple_2d_data)

        assert gmm.means_.shape == (n_components, n_features)
        assert gmm.covariances_.shape == (n_components, n_features, n_features)
        assert gmm.weights_.shape == (n_components,)

    def test_fit_weights_sum_to_one(self, simple_2d_data):
        """Test that weights sum to approximately 1."""
        gmm = TorchGMM(n_components=2)
        gmm.fit(simple_2d_data)

        assert np.allclose(gmm.weights_.sum(), 1.0, atol=1e-5)

    def test_fit_weights_positive(self, simple_2d_data):
        """Test that all weights are positive."""
        gmm = TorchGMM(n_components=2)
        gmm.fit(simple_2d_data)

        assert np.all(gmm.weights_ > 0)

    def test_fit_covariances_positive_definite(self, simple_2d_data):
        """Test that covariance matrices are positive definite."""
        gmm = TorchGMM(n_components=2)
        gmm.fit(simple_2d_data)

        for cov in gmm.covariances_:
            eigenvalues = np.linalg.eigvalsh(cov)
            assert np.all(eigenvalues > 0), "Covariance matrix is not positive definite"

    def test_fit_with_custom_means_init(self, simple_2d_data):
        """Test fitting with custom mean initialization."""
        means_init = np.array([[0.0, 0.0], [5.0, 5.0]])
        gmm = TorchGMM(n_components=2, means_init=means_init)
        gmm.fit(simple_2d_data)

        # Means should be close to initialized values for well-separated clusters
        # Check that at least one mean is close to each initialization
        distances = np.linalg.norm(gmm.means_[:, None] - means_init[None, :], axis=2)
        assert np.min(distances, axis=1).max() < 1.0

    def test_fit_invalid_data_shape(self):
        """Test that 1D or 3D+ data raises ValueError."""
        gmm = TorchGMM(n_components=2)

        with pytest.raises(ValueError, match="Input data must be 2D"):
            gmm.fit(np.random.randn(100))  # 1D data

        with pytest.raises(ValueError, match="Input data must be 2D"):
            gmm.fit(np.random.randn(10, 5, 2))  # 3D data

    def test_fit_wrong_means_init_shape(self):
        """Test that incorrect means_init shape raises ValueError."""
        gmm = TorchGMM(n_components=2, means_init=np.random.randn(3, 2))
        data = np.random.randn(100, 2)

        with pytest.raises(ValueError, match="means_init must have shape"):
            gmm.fit(data)

    def test_fit_convergence(self, simple_2d_data):
        """Test that fitting converges within max_iter."""
        gmm = TorchGMM(n_components=2, max_iter=100, tol=1e-3)
        gmm.fit(simple_2d_data)

        # Just verify it completes without error
        assert gmm.means_ is not None

    def test_fit_3d_data(self, simple_3d_data):
        """Test fitting on 3D data."""
        gmm = TorchGMM(n_components=3)
        gmm.fit(simple_3d_data)

        assert gmm.means_.shape == (3, 3)
        assert gmm.covariances_.shape == (3, 3, 3)
        assert gmm.weights_.shape == (3,)

    def test_fit_regularization(self):
        """Test that regularization prevents singular covariance matrices."""
        # Create data that could lead to singular covariance
        data = np.random.randn(50, 2)
        data[:, 1] = data[:, 0]  # Perfect correlation

        gmm = TorchGMM(n_components=1, reg_covar=1e-3)
        gmm.fit(data)

        # Should not raise error and covariance should be positive definite
        eigenvalues = np.linalg.eigvalsh(gmm.covariances_[0])
        assert np.all(eigenvalues > 0)

    @pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")
    def test_fit_on_gpu(self, simple_2d_data):
        """Test fitting on GPU."""
        gmm = TorchGMM(n_components=2, device="cuda")
        gmm.fit(simple_2d_data)

        assert gmm.means_ is not None
        assert gmm.means_.shape == (2, 2)


class TestTorchGMMPredictProba:
    """Test TorchGMM probability prediction."""

    @pytest.fixture
    def fitted_gmm(self):
        """Create a fitted GMM on simple 2-cluster data."""
        np.random.seed(42)
        cluster1 = np.random.randn(100, 2) * 0.5 + np.array([0, 0])
        cluster2 = np.random.randn(100, 2) * 0.5 + np.array([5, 5])
        data = np.vstack([cluster1, cluster2])

        gmm = TorchGMM(n_components=2)
        gmm.fit(data)
        return gmm

    def test_predict_proba_shape(self, fitted_gmm):
        """Test that predict_proba returns correct shape."""
        test_data = np.random.randn(50, 2)
        proba = fitted_gmm.predict_proba(test_data)

        assert proba.shape == (50, 2)

    def test_predict_proba_sum_to_one(self, fitted_gmm):
        """Test that probabilities sum to 1 for each sample."""
        test_data = np.random.randn(50, 2)
        proba = fitted_gmm.predict_proba(test_data)

        row_sums = proba.sum(axis=1)
        assert np.allclose(row_sums, 1.0, atol=1e-5)

    def test_predict_proba_range(self, fitted_gmm):
        """Test that probabilities are in [0, 1]."""
        test_data = np.random.randn(50, 2)
        proba = fitted_gmm.predict_proba(test_data)

        assert np.all(proba >= 0)
        assert np.all(proba <= 1)

    def test_predict_proba_returns_numpy(self, fitted_gmm):
        """Test that predict_proba returns NumPy array."""
        test_data = np.random.randn(50, 2)
        proba = fitted_gmm.predict_proba(test_data)

        assert isinstance(proba, np.ndarray)

    def test_predict_proba_cluster_assignment(self, fitted_gmm):
        """Test that points near cluster centers get high probability."""
        # Points near first cluster center
        near_cluster1 = fitted_gmm.means_[0:1] + np.random.randn(10, 2) * 0.1
        proba1 = fitted_gmm.predict_proba(near_cluster1)

        # Points near second cluster center
        near_cluster2 = fitted_gmm.means_[1:2] + np.random.randn(10, 2) * 0.1
        proba2 = fitted_gmm.predict_proba(near_cluster2)

        # Check that probabilities are high for the correct cluster
        assert np.mean(proba1[:, 0]) > 0.7 or np.mean(proba1[:, 1]) > 0.7
        assert np.mean(proba2[:, 0]) > 0.7 or np.mean(proba2[:, 1]) > 0.7

    def test_predict_proba_single_point(self, fitted_gmm):
        """Test prediction on a single data point."""
        point = np.random.randn(1, 2)
        proba = fitted_gmm.predict_proba(point)

        assert proba.shape == (1, 2)
        assert np.allclose(proba.sum(), 1.0)

    def test_predict_proba_with_torch_tensor(self, fitted_gmm):
        """Test that predict_proba works with torch tensor input."""
        test_data = torch.randn(50, 2)
        proba = fitted_gmm.predict_proba(test_data)

        assert isinstance(proba, np.ndarray)
        assert proba.shape == (50, 2)


class TestTorchGMMEdgeCases:
    """Test edge cases and error handling."""

    def test_single_component(self):
        """Test GMM with single component."""
        np.random.seed(42)
        data = np.random.randn(100, 2)

        gmm = TorchGMM(n_components=1)
        gmm.fit(data)

        assert gmm.means_.shape == (1, 2)
        assert gmm.weights_.shape == (1,)
        assert np.allclose(gmm.weights_[0], 1.0)

    def test_many_components(self):
        """Test GMM with many components relative to data size."""
        np.random.seed(42)
        data = np.random.randn(50, 2)

        gmm = TorchGMM(n_components=10)
        gmm.fit(data)

        assert gmm.means_.shape == (10, 2)
        assert gmm.covariances_.shape == (10, 2, 2)
        assert np.allclose(gmm.weights_.sum(), 1.0)

    def test_high_dimensional_data(self):
        """Test GMM on high-dimensional data."""
        np.random.seed(42)
        data = np.random.randn(200, 20)

        gmm = TorchGMM(n_components=3, reg_covar=1e-3)
        gmm.fit(data)

        assert gmm.means_.shape == (3, 20)
        assert gmm.covariances_.shape == (3, 20, 20)

    def test_minimal_data(self):
        """Test GMM with minimal amount of data."""
        data = np.random.randn(5, 2)

        gmm = TorchGMM(n_components=2, reg_covar=1e-2)
        gmm.fit(data)

        assert gmm.means_.shape == (2, 2)

    def test_identical_data_points(self):
        """Test GMM when all data points are identical."""
        data = np.ones((50, 2))

        gmm = TorchGMM(n_components=2, reg_covar=1e-1)
        gmm.fit(data)

        # Should still fit without error due to regularization
        assert gmm.means_.shape == (2, 2)
        # Means should be close to the identical point
        assert np.allclose(gmm.means_, 1.0, atol=1.0)

    def test_early_convergence(self):
        """Test that fitting stops early if converged."""
        np.random.seed(42)
        # Well-separated clusters should converge quickly
        cluster1 = np.random.randn(100, 2) * 0.3 + np.array([0, 0])
        cluster2 = np.random.randn(100, 2) * 0.3 + np.array([10, 10])
        data = np.vstack([cluster1, cluster2])

        gmm = TorchGMM(n_components=2, max_iter=1000, tol=1e-3)
        gmm.fit(data)

        # Should converge (just verify no error)
        assert gmm.means_ is not None

    def test_zero_regularization(self):
        """Test with zero regularization on well-conditioned data."""
        np.random.seed(42)
        data = np.random.randn(100, 2)

        gmm = TorchGMM(n_components=2, reg_covar=0.0)
        gmm.fit(data)

        assert gmm.means_ is not None

    def test_very_small_tolerance(self):
        """Test with very small tolerance."""
        np.random.seed(42)
        data = np.random.randn(100, 2)

        gmm = TorchGMM(n_components=2, tol=1e-10, max_iter=50)
        gmm.fit(data)

        assert gmm.means_ is not None

    def test_very_large_tolerance(self):
        """Test with very large tolerance (should converge in 1-2 iterations)."""
        np.random.seed(42)
        data = np.random.randn(100, 2)

        gmm = TorchGMM(n_components=2, tol=1e1, max_iter=100)
        gmm.fit(data)

        assert gmm.means_ is not None


class TestTorchGMMInternalMethods:
    """Test internal methods of TorchGMM."""

    @pytest.fixture
    def gmm_with_data(self):
        """Create GMM and data for testing internal methods."""
        np.random.seed(42)
        data = np.random.randn(100, 2)
        gmm = TorchGMM(n_components=2)
        X = gmm._to_tensor(data)
        gmm._init_params(X)
        return gmm, X

    def test_init_params_creates_tensors(self, gmm_with_data):
        """Test that _init_params creates internal tensors."""
        gmm, _ = gmm_with_data

        assert isinstance(gmm._means, torch.Tensor)
        assert isinstance(gmm._covariances, torch.Tensor)
        assert isinstance(gmm._weights, torch.Tensor)

    def test_init_params_correct_shapes(self, gmm_with_data):
        """Test that _init_params creates tensors with correct shapes."""
        gmm, X = gmm_with_data
        N, D = X.shape
        K = gmm.n_components

        assert gmm._means.shape == (K, D)
        assert gmm._covariances.shape == (K, D, D)
        assert gmm._weights.shape == (K,)

    def test_init_params_with_means_init(self):
        """Test _init_params with custom mean initialization."""
        means_init = np.array([[0.0, 0.0], [1.0, 1.0]])
        gmm = TorchGMM(n_components=2, means_init=means_init)
        data = np.random.randn(100, 2)
        X = gmm._to_tensor(data)
        gmm._init_params(X)

        assert torch.allclose(gmm._means, gmm._to_tensor(means_init), atol=1e-5)

    def test_log_gaussians_shape(self, gmm_with_data):
        """Test that _log_gaussians returns correct shape."""
        gmm, X = gmm_with_data
        log_comp = gmm._log_gaussians(X)

        N = X.shape[0]
        K = gmm.n_components
        assert log_comp.shape == (N, K)

    def test_log_gaussians_finite(self, gmm_with_data):
        """Test that _log_gaussians returns finite values."""
        gmm, X = gmm_with_data
        log_comp = gmm._log_gaussians(X)

        assert torch.all(torch.isfinite(log_comp))

    def test_e_step_returns_valid_responsibilities(self, gmm_with_data):
        """Test that _e_step returns valid responsibilities."""
        gmm, X = gmm_with_data
        r, log_post = gmm._e_step(X)

        N = X.shape[0]
        K = gmm.n_components

        # Check shapes
        assert r.shape == (N, K)
        assert log_post.shape == (N, K)

        # Check that responsibilities sum to 1
        row_sums = r.sum(dim=1)
        assert torch.allclose(row_sums, torch.ones_like(row_sums), atol=1e-5)

        # Check that responsibilities are in [0, 1]
        assert torch.all(r >= 0)
        assert torch.all(r <= 1)

    def test_m_step_updates_parameters(self, gmm_with_data):
        """Test that _m_step updates parameters."""
        gmm, X = gmm_with_data

        # Store initial parameters
        initial_means = gmm._means.clone()
        initial_weights = gmm._weights.clone()

        # Run E-step and M-step
        r, _ = gmm._e_step(X)
        gmm._m_step(X, r)

        # Parameters should change (unless already converged)
        # Just check that shapes are preserved
        assert gmm._means.shape == initial_means.shape
        assert gmm._weights.shape == initial_weights.shape

    def test_m_step_maintains_weight_sum(self, gmm_with_data):
        """Test that _m_step maintains weight sum = 1."""
        gmm, X = gmm_with_data

        r, _ = gmm._e_step(X)
        gmm._m_step(X, r)

        assert torch.allclose(gmm._weights.sum(), torch.tensor(1.0), atol=1e-5)

    def test_m_step_positive_definite_covariances(self, gmm_with_data):
        """Test that _m_step produces positive definite covariances."""
        gmm, X = gmm_with_data

        r, _ = gmm._e_step(X)
        gmm._m_step(X, r)

        # Check each covariance matrix is positive definite
        for k in range(gmm.n_components):
            cov = gmm._covariances[k].cpu().numpy()
            eigenvalues = np.linalg.eigvalsh(cov)
            assert np.all(eigenvalues > 0), f"Covariance {k} is not positive definite"


class TestTorchGMMReproducibility:
    """Test reproducibility and consistency."""

    def test_same_seed_same_results(self):
        """Test that same random seed gives same results."""
        data = np.random.randn(100, 2)

        # First fit
        torch.manual_seed(42)
        np.random.seed(42)
        gmm1 = TorchGMM(n_components=2)
        gmm1.fit(data)

        # Second fit with same seed
        torch.manual_seed(42)
        np.random.seed(42)
        gmm2 = TorchGMM(n_components=2)
        gmm2.fit(data)

        # Results should be identical
        assert np.allclose(gmm1.means_, gmm2.means_, atol=1e-5)
        assert np.allclose(gmm1.covariances_, gmm2.covariances_, atol=1e-5)
        assert np.allclose(gmm1.weights_, gmm2.weights_, atol=1e-5)

    def test_predict_proba_deterministic(self):
        """Test that predict_proba is deterministic for fitted model."""
        np.random.seed(42)
        data = np.random.randn(100, 2)

        gmm = TorchGMM(n_components=2)
        gmm.fit(data)

        test_data = np.random.randn(50, 2)
        proba1 = gmm.predict_proba(test_data)
        proba2 = gmm.predict_proba(test_data)

        assert np.allclose(proba1, proba2, atol=1e-6)

    def test_refitting_changes_results(self):
        """Test that refitting with different seed gives different results."""
        data = np.random.randn(100, 2)

        # First fit
        torch.manual_seed(42)
        np.random.seed(42)
        gmm1 = TorchGMM(n_components=2)
        gmm1.fit(data)

        # Second fit with different seed
        torch.manual_seed(123)
        np.random.seed(123)
        gmm2 = TorchGMM(n_components=2)
        gmm2.fit(data)

        # Results should be different (unless extremely unlikely)
        assert not np.allclose(gmm1.means_, gmm2.means_, atol=1e-3)


class TestTorchGMMDtypeAndDevice:
    """Test dtype and device handling."""

    @pytest.mark.parametrize("dtype", [torch.float32, torch.float64])
    def test_different_dtypes(self, dtype):
        """Test GMM with different data types."""
        data = np.random.randn(100, 2)

        gmm = TorchGMM(n_components=2, dtype=dtype)
        gmm.fit(data)

        assert gmm._means.dtype == dtype
        assert gmm._covariances.dtype == dtype
        assert gmm._weights.dtype == dtype

    def test_cpu_device(self):
        """Test GMM on CPU."""
        data = np.random.randn(100, 2)

        gmm = TorchGMM(n_components=2, device="cpu")
        gmm.fit(data)

        assert gmm._means.device.type == "cpu"
        assert gmm._covariances.device.type == "cpu"
        assert gmm._weights.device.type == "cpu"

    @pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")
    def test_cuda_device(self):
        """Test GMM on CUDA."""
        data = np.random.randn(100, 2)

        gmm = TorchGMM(n_components=2, device="cuda")
        gmm.fit(data)

        assert gmm._means.device.type == "cuda"
        assert gmm._covariances.device.type == "cuda"
        assert gmm._weights.device.type == "cuda"

    @pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")
    def test_cpu_gpu_consistency(self):
        """Test that CPU and GPU give similar results."""
        np.random.seed(42)
        torch.manual_seed(42)
        data = np.random.randn(100, 2)

        # Fit on CPU
        torch.manual_seed(42)
        gmm_cpu = TorchGMM(n_components=2, device="cpu")
        gmm_cpu.fit(data)

        # Fit on GPU
        torch.manual_seed(42)
        gmm_gpu = TorchGMM(n_components=2, device="cuda")
        gmm_gpu.fit(data)

        # Results should be very similar
        assert np.allclose(gmm_cpu.means_, gmm_gpu.means_, atol=1e-4)
        assert np.allclose(gmm_cpu.weights_, gmm_gpu.weights_, atol=1e-4)


class TestTorchGMMNumericalStability:
    """Test numerical stability and handling of extreme cases."""

    def test_very_small_variance(self):
        """Test GMM with data having very small variance."""
        data = np.random.randn(100, 2) * 1e-6

        gmm = TorchGMM(n_components=2, reg_covar=1e-8)
        gmm.fit(data)

        assert np.all(np.isfinite(gmm.means_))
        assert np.all(np.isfinite(gmm.covariances_))

    def test_very_large_values(self):
        """Test GMM with very large data values."""
        data = np.random.randn(100, 2) * 1e6

        gmm = TorchGMM(n_components=2, reg_covar=1e3)
        gmm.fit(data)

        assert np.all(np.isfinite(gmm.means_))
        assert np.all(np.isfinite(gmm.covariances_))
        assert np.all(np.isfinite(gmm.weights_))

    def test_mixed_scale_features(self):
        """Test GMM with features on very different scales."""
        np.random.seed(42)
        data = np.column_stack(
            [
                np.random.randn(100) * 1e-3,  # Small scale
                np.random.randn(100) * 1e3,  # Large scale
            ]
        )

        gmm = TorchGMM(n_components=2, reg_covar=1e-3)
        gmm.fit(data)

        assert np.all(np.isfinite(gmm.means_))
        assert np.all(np.isfinite(gmm.covariances_))

    def test_near_singular_covariance(self):
        """Test GMM when data nearly lies on a line."""
        np.random.seed(42)
        # Data nearly on a line y = x
        x = np.random.randn(100)
        y = x + np.random.randn(100) * 1e-6
        data = np.column_stack([x, y])

        gmm = TorchGMM(n_components=2, reg_covar=1e-4)
        gmm.fit(data)

        # Should not raise error due to regularization
        assert np.all(np.isfinite(gmm.covariances_))

        # Check positive definiteness
        for cov in gmm.covariances_:
            eigenvalues = np.linalg.eigvalsh(cov)
            assert np.all(eigenvalues > 0)

    def test_outliers_present(self):
        """Test GMM with outliers in the data."""
        np.random.seed(42)
        # Main cluster
        main_data = np.random.randn(95, 2)
        # Outliers
        outliers = np.random.randn(5, 2) * 10 + 20
        data = np.vstack([main_data, outliers])

        gmm = TorchGMM(n_components=2)
        gmm.fit(data)

        assert np.all(np.isfinite(gmm.means_))
        assert np.all(np.isfinite(gmm.covariances_))

    def test_infinite_log_likelihood_handling(self):
        """Test that initial -inf log-likelihood is handled correctly."""
        data = np.random.randn(100, 2)

        gmm = TorchGMM(n_components=2, max_iter=5)
        gmm.fit(data)

        # Should complete without error even with initial -inf
        assert gmm.means_ is not None

    def test_nan_check_in_data(self):
        """Test that NaN in data causes issues (expected behavior)."""
        data = np.random.randn(100, 2)
        data[50, 0] = np.nan

        gmm = TorchGMM(n_components=2)
        # This may raise an error or produce NaN results
        # depending on implementation - we just verify it doesn't crash silently
        try:
            gmm.fit(data)
            # If it doesn't raise, check for NaN in output
            if gmm.means_ is not None:
                has_nan = np.any(np.isnan(gmm.means_))
                # Either raises error or produces NaN - both are acceptable
                assert has_nan or True
        except (ValueError, RuntimeError):
            # Expected behavior for NaN input
            pass


class TestTorchGMMComparisonWithSklearn:
    """Test TorchGMM produces similar results to sklearn (if available)."""

    @pytest.fixture(autouse=True)
    def check_sklearn(self):
        """Skip tests if sklearn is not available."""
        pytest.importorskip("sklearn")

    def test_similar_to_sklearn_simple_case(self):
        """Test that TorchGMM gives similar results to sklearn GMM."""
        from sklearn.mixture import GaussianMixture

        np.random.seed(42)
        torch.manual_seed(42)

        # Create well-separated clusters
        cluster1 = np.random.randn(100, 2) * 0.5 + np.array([0, 0])
        cluster2 = np.random.randn(100, 2) * 0.5 + np.array([5, 5])
        data = np.vstack([cluster1, cluster2])

        # Fit with sklearn
        sk_gmm = GaussianMixture(
            n_components=2,
            covariance_type="full",
            random_state=42,
            max_iter=200,
            tol=1e-4,
            reg_covar=1e-6,
        )
        sk_gmm.fit(data)

        # Fit with TorchGMM
        torch.manual_seed(42)
        np.random.seed(42)
        torch_gmm = TorchGMM(n_components=2, max_iter=200, tol=1e-4, reg_covar=1e-6)
        torch_gmm.fit(data)

        # Weights should sum to 1 for both
        assert np.allclose(sk_gmm.weights_.sum(), 1.0)
        assert np.allclose(torch_gmm.weights_.sum(), 1.0)

        # Shapes should match
        assert sk_gmm.means_.shape == torch_gmm.means_.shape
        assert sk_gmm.covariances_.shape == torch_gmm.covariances_.shape

    def test_similar_predictions_to_sklearn(self):
        """Test that predict_proba gives similar results to sklearn."""
        from sklearn.mixture import GaussianMixture

        np.random.seed(42)
        torch.manual_seed(42)

        # Training data
        data = np.random.randn(100, 2)

        # Fit both models with same initialization
        means_init = data[np.random.choice(100, 2, replace=False)]

        sk_gmm = GaussianMixture(
            n_components=2, covariance_type="full", means_init=means_init, random_state=42
        )
        sk_gmm.fit(data)

        torch_gmm = TorchGMM(n_components=2, means_init=means_init)
        torch.manual_seed(42)
        torch_gmm.fit(data)

        # Test data
        test_data = np.random.randn(50, 2)

        sk_proba = sk_gmm.predict_proba(test_data)
        torch_proba = torch_gmm.predict_proba(test_data)

        # Both should sum to 1
        assert np.allclose(sk_proba.sum(axis=1), 1.0)
        assert np.allclose(torch_proba.sum(axis=1), 1.0)


class TestTorchGMMMethodChaining:
    """Test method chaining and fluent interface."""

    def test_fit_returns_self_for_chaining(self):
        """Test that fit returns self to enable chaining."""
        data = np.random.randn(100, 2)

        gmm = TorchGMM(n_components=2)
        result = gmm.fit(data)

        assert result is gmm
        assert result.means_ is not None

    def test_chained_fit_predict_proba(self):
        """Test chaining fit and predict_proba."""
        train_data = np.random.randn(100, 2)
        test_data = np.random.randn(50, 2)

        proba = TorchGMM(n_components=2).fit(train_data).predict_proba(test_data)

        assert proba.shape == (50, 2)
        assert np.allclose(proba.sum(axis=1), 1.0)


class TestTorchGMMMemoryManagement:
    """Test memory management and cleanup."""

    def test_multiple_fits_same_object(self):
        """Test that fitting multiple times on same object works."""
        gmm = TorchGMM(n_components=2)

        data1 = np.random.randn(100, 2)
        gmm.fit(data1)
        means1 = gmm.means_.copy()

        data2 = np.random.randn(100, 2) + 5
        gmm.fit(data2)
        means2 = gmm.means_.copy()

        # Means should be different after refitting
        assert not np.allclose(means1, means2, atol=0.5)

    def test_internal_tensors_updated(self):
        """Test that internal tensors are properly updated."""
        gmm = TorchGMM(n_components=2)
        data = np.random.randn(100, 2)

        gmm.fit(data)

        # Internal tensors should exist
        assert gmm._means is not None
        assert gmm._covariances is not None
        assert gmm._weights is not None

        # External arrays should exist
        assert gmm.means_ is not None
        assert gmm.covariances_ is not None
        assert gmm.weights_ is not None

    @pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")
    def test_gpu_memory_cleanup(self):
        """Test that GPU memory is managed properly."""
        data = np.random.randn(1000, 10)

        initial_memory = torch.cuda.memory_allocated()

        gmm = TorchGMM(n_components=5, device="cuda")
        gmm.fit(data)

        # Memory should be allocated
        assert torch.cuda.memory_allocated() > initial_memory

        # Cleanup
        del gmm
        torch.cuda.empty_cache()


class TestTorchGMMSpecialCases:
    """Test special mathematical cases."""

    def test_perfect_separation(self):
        """Test GMM with perfectly separated clusters."""
        np.random.seed(42)
        cluster1 = np.random.randn(50, 2) * 0.1 + np.array([0, 0])
        cluster2 = np.random.randn(50, 2) * 0.1 + np.array([100, 100])
        data = np.vstack([cluster1, cluster2])

        gmm = TorchGMM(n_components=2)
        gmm.fit(data)

        # Should find two well-separated means
        mean_distance = np.linalg.norm(gmm.means_[0] - gmm.means_[1])
        assert mean_distance > 50

    def test_overlapping_clusters(self):
        """Test GMM with heavily overlapping clusters."""
        np.random.seed(42)
        cluster1 = np.random.randn(100, 2) * 2 + np.array([0, 0])
        cluster2 = np.random.randn(100, 2) * 2 + np.array([0.5, 0.5])
        data = np.vstack([cluster1, cluster2])

        gmm = TorchGMM(n_components=2)
        gmm.fit(data)

        # Should still fit without error
        assert gmm.means_ is not None
        assert np.allclose(gmm.weights_.sum(), 1.0)

    def test_unbalanced_clusters(self):
        """Test GMM with very unbalanced cluster sizes."""
        np.random.seed(42)
        cluster1 = np.random.randn(10, 2) * 0.5
        cluster2 = np.random.randn(190, 2) * 0.5 + np.array([3, 3])
        data = np.vstack([cluster1, cluster2])

        gmm = TorchGMM(n_components=2)
        gmm.fit(data)

        # Weights should reflect the imbalance
        assert gmm.weights_.min() < 0.3  # Smaller cluster has less weight
        assert gmm.weights_.max() > 0.7  # Larger cluster has more weight

    def test_spherical_vs_elongated_clusters(self):
        """Test GMM with clusters of different shapes."""
        np.random.seed(42)
        # Spherical cluster
        cluster1 = np.random.randn(100, 2) * 0.5
        # Elongated cluster
        cluster2 = np.random.randn(100, 2) * np.array([2.0, 0.2]) + np.array([5, 5])
        data = np.vstack([cluster1, cluster2])

        gmm = TorchGMM(n_components=2)
        gmm.fit(data)

        # Different covariances should be captured
        cov1_det = np.linalg.det(gmm.covariances_[0])
        cov2_det = np.linalg.det(gmm.covariances_[1])

        # Determinants should be different (though order may vary)
        assert not np.allclose(cov1_det, cov2_det, rtol=0.5)


# Performance and stress tests (optional, can be slow)
class TestTorchGMMPerformance:
    """Performance and stress tests."""

    @pytest.mark.slow
    def test_large_dataset(self):
        """Test GMM on large dataset."""
        data = np.random.randn(10000, 5)

        gmm = TorchGMM(n_components=10, max_iter=50)
        gmm.fit(data)

        assert gmm.means_.shape == (10, 5)

    @pytest.mark.slow
    def test_many_iterations(self):
        """Test GMM with many iterations."""
        data = np.random.randn(200, 2)

        gmm = TorchGMM(n_components=3, max_iter=1000, tol=1e-8)
        gmm.fit(data)

        assert gmm.means_ is not None

    @pytest.mark.slow
    @pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")
    def test_gpu_performance(self):
        """Test that GPU version runs without error on large data."""
        data = np.random.randn(5000, 10)

        gmm = TorchGMM(n_components=20, device="cuda", max_iter=50)
        gmm.fit(data)

        assert gmm.means_.shape == (20, 10)

    @pytest.mark.slow
    def test_high_dimensional_performance(self):
        """Test GMM on high-dimensional data."""
        data = np.random.randn(500, 50)

        gmm = TorchGMM(n_components=5, max_iter=30, reg_covar=1e-3)
        gmm.fit(data)

        assert gmm.means_.shape == (5, 50)
        assert gmm.covariances_.shape == (5, 50, 50)


class TestTorchGMMDocstringExamples:
    """Test examples that might appear in documentation."""

    def test_basic_usage_example(self):
        """Test basic usage example."""
        # Generate sample data
        np.random.seed(42)
        X = np.vstack([np.random.randn(100, 2) * 0.5, np.random.randn(100, 2) * 0.5 + [3, 3]])

        # Fit GMM
        gmm = TorchGMM(n_components=2)
        gmm.fit(X)

        # Get cluster probabilities
        probabilities = gmm.predict_proba(X)

        assert probabilities.shape == (200, 2)
        assert gmm.means_.shape == (2, 2)

    def test_custom_initialization_example(self):
        """Test example with custom initialization."""
        np.random.seed(42)
        X = np.random.randn(100, 2)

        # Custom initial means
        means_init = np.array([[0, 0], [1, 1]])

        gmm = TorchGMM(n_components=2, means_init=means_init)
        gmm.fit(X)

        assert gmm.means_ is not None

    def test_gpu_usage_example(self):
        """Test example of GPU usage."""
        if not torch.cuda.is_available():
            pytest.skip("CUDA not available")

        np.random.seed(42)
        X = np.random.randn(100, 2)

        gmm = TorchGMM(n_components=2, device="cuda")
        gmm.fit(X)

        assert gmm.means_ is not None

    def test_method_chaining_example(self):
        """Test method chaining example."""
        np.random.seed(42)
        X_train = np.random.randn(100, 2)
        X_test = np.random.randn(50, 2)

        # Fit and predict in one line
        probabilities = TorchGMM(n_components=2).fit(X_train).predict_proba(X_test)

        assert probabilities.shape == (50, 2)


class TestTorchGMMRobustness:
    """Test robustness to various edge cases and unusual inputs."""

    def test_empty_after_init(self):
        """Test that newly initialized GMM has None attributes."""
        gmm = TorchGMM(n_components=3)

        assert gmm.means_ is None
        assert gmm.covariances_ is None
        assert gmm.weights_ is None

    def test_non_contiguous_array(self):
        """Test with non-contiguous numpy array."""
        data = np.random.randn(100, 10)
        # Create non-contiguous view
        data_nc = data[:, ::2]

        gmm = TorchGMM(n_components=2)
        gmm.fit(data_nc)

        assert gmm.means_.shape == (2, 5)

    def test_fortran_ordered_array(self):
        """Test with Fortran-ordered array."""
        data = np.asfortranarray(np.random.randn(100, 2))

        gmm = TorchGMM(n_components=2)
        gmm.fit(data)

        assert gmm.means_ is not None

    def test_integer_input_data(self):
        """Test with integer input data."""
        data = np.random.randint(-10, 10, size=(100, 2))

        gmm = TorchGMM(n_components=2)
        gmm.fit(data)

        assert gmm.means_ is not None
        assert gmm.means_.dtype == np.float32

    def test_mixed_type_input(self):
        """Test with mixed int/float data."""
        data = np.column_stack([np.random.randint(0, 10, 100), np.random.randn(100)])

        gmm = TorchGMM(n_components=2)
        gmm.fit(data)

        assert gmm.means_ is not None

    def test_predict_before_fit_fails_gracefully(self):
        """Test that predict_proba before fit raises or returns sensible error."""
        gmm = TorchGMM(n_components=2)
        data = np.random.randn(10, 2)

        # Should fail because model is not fitted
        with pytest.raises((AttributeError, RuntimeError, TypeError)):
            gmm.predict_proba(data)

    def test_very_few_samples_per_component(self):
        """Test when n_samples < n_components."""
        data = np.random.randn(3, 2)

        gmm = TorchGMM(n_components=5, reg_covar=1e-2)
        gmm.fit(data)

        # Should still fit with regularization
        assert gmm.means_ is not None

    def test_single_sample(self):
        """Test with just one sample."""
        data = np.array([[1.0, 2.0]])

        gmm = TorchGMM(n_components=1, reg_covar=1e-1)
        gmm.fit(data)

        assert gmm.means_.shape == (1, 2)
        assert np.allclose(gmm.means_[0], [1.0, 2.0], atol=0.1)

    def test_constant_feature(self):
        """Test with one constant feature."""
        data = np.random.randn(100, 2)
        data[:, 1] = 5.0  # Constant second feature

        gmm = TorchGMM(n_components=2, reg_covar=1e-3)
        gmm.fit(data)

        assert gmm.means_ is not None
        # All means should have second feature â‰ˆ 5
        assert np.allclose(gmm.means_[:, 1], 5.0, atol=0.1)


class TestTorchGMMParameterValidation:
    """Test parameter validation and error messages."""

    def test_negative_n_components(self):
        """Test that negative n_components is handled."""
        # May raise ValueError or get converted to positive
        try:
            gmm = TorchGMM(n_components=-2)
            # If it doesn't raise, check it's been converted
            assert gmm.n_components >= 0
        except (ValueError, AssertionError):
            pass

    def test_zero_n_components(self):
        """Test with zero components."""
        gmm = TorchGMM(n_components=0)
        data = np.random.randn(100, 2)

        # Should fail to fit
        with pytest.raises((ValueError, RuntimeError, IndexError)):
            gmm.fit(data)

    def test_negative_tolerance(self):
        """Test that negative tolerance is converted to positive."""
        gmm = TorchGMM(n_components=2, tol=-1e-4)
        # Should either raise or convert to positive
        assert gmm.tol >= 0 or True  # Accept either behavior

    def test_negative_max_iter(self):
        """Test that negative max_iter is handled."""
        gmm = TorchGMM(n_components=2, max_iter=-10)
        # Should convert to non-negative
        assert gmm.max_iter >= 0

    def test_wrong_means_init_dimensions(self):
        """Test with wrong number of features in means_init."""
        means_init = np.array([[0, 0, 0], [1, 1, 1]])  # 3 features
        gmm = TorchGMM(n_components=2, means_init=means_init)
        data = np.random.randn(100, 2)  # 2 features

        with pytest.raises(ValueError, match="means_init must have shape"):
            gmm.fit(data)

    def test_wrong_means_init_n_components(self):
        """Test with wrong number of components in means_init."""
        means_init = np.array([[0, 0], [1, 1], [2, 2]])  # 3 components
        gmm = TorchGMM(n_components=2, means_init=means_init)
        data = np.random.randn(100, 2)

        with pytest.raises(ValueError, match="means_init must have shape"):
            gmm.fit(data)


class TestTorchGMMAttributeAccess:
    """Test attribute access patterns."""

    def test_accessing_fitted_attributes_before_fit(self):
        """Test that attributes are None before fitting."""
        gmm = TorchGMM(n_components=2)

        assert gmm.means_ is None
        assert gmm.covariances_ is None
        assert gmm.weights_ is None

    def test_accessing_internal_attributes_before_fit(self):
        """Test that internal attributes are None before fitting."""
        gmm = TorchGMM(n_components=2)

        assert gmm._means is None
        assert gmm._covariances is None
        assert gmm._weights is None

    def test_fitted_attributes_are_numpy(self):
        """Test that public attributes are NumPy arrays."""
        data = np.random.randn(100, 2)
        gmm = TorchGMM(n_components=2)
        gmm.fit(data)

        assert isinstance(gmm.means_, np.ndarray)
        assert isinstance(gmm.covariances_, np.ndarray)
        assert isinstance(gmm.weights_, np.ndarray)

    def test_internal_attributes_are_torch(self):
        """Test that internal attributes are torch tensors."""
        data = np.random.randn(100, 2)
        gmm = TorchGMM(n_components=2)
        gmm.fit(data)

        assert isinstance(gmm._means, torch.Tensor)
        assert isinstance(gmm._covariances, torch.Tensor)
        assert isinstance(gmm._weights, torch.Tensor)

    def test_modifying_public_attributes_doesnt_affect_internal(self):
        """Test that modifying public attrs doesn't affect internal state."""
        data = np.random.randn(100, 2)
        gmm = TorchGMM(n_components=2)
        gmm.fit(data)

        original_means = gmm._means.clone()

        # Modify public attribute
        gmm.means_[0, 0] = 999.0

        # Internal should be unchanged
        assert torch.allclose(gmm._means, original_means)


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
